{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "[[1 5 8 3]\n",
      " [4 7 8 9]]\n",
      "[[1 4]\n",
      " [5 7]\n",
      " [8 8]\n",
      " [3 9]]\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "the_data = np.array([1,5,7,8,9])\n",
    "\n",
    "print (np.mean(the_data))\n",
    "\n",
    "the_data_2d = np.array([(1,5,8,3), (4,7,8,9)])\n",
    "\n",
    "print (the_data_2d)\n",
    "print (np.transpose(the_data_2d))\n",
    "print (the_data_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fishing', 'hiking', 'machinelearning', 'mathematics']\n",
      "                                                body    label\n",
      "0  DNR - Weekly Fishing Report\\nDNR Home Contact ...  fishing\n",
      "1  DNR: Fishing Guide & Regulations\\nÃ Ã Header...  fishing\n",
      "2  NH Hunting and Fishing Licenses | New Hampshir...  fishing\n",
      "3  46 Bait & Tackle - CLOSED - Outdoor Gear - 22 ...  fishing\n",
      "4  5 rescued from capsized sport fishing boat at ...  fishing\n"
     ]
    }
   ],
   "source": [
    "#recall from last week\n",
    "import os\n",
    "import pandas as pd\n",
    "the_path = 'C:/Users/pathouli/myStuff/academia/columbia/socialSciences/GR5067/data/topics/'\n",
    "the_dirs = os.listdir(the_path)\n",
    "print (the_dirs)\n",
    "\n",
    "the_files = os.listdir(the_path + the_dirs[0])\n",
    "the_file_names = pd.DataFrame()\n",
    "for word in the_files:\n",
    "    the_file_names = the_file_names.append({'file_name': word, 'label': the_dirs[0]}, ignore_index=True)\n",
    "    \n",
    "\n",
    "#IN-CLASS EXERCISE; in lieu of the filename, do the same as above, but just read the text in\n",
    "#and CLEAN it up, below is starter code to get you pointed in the right direction\n",
    "import os\n",
    "import pandas as pd\n",
    "t_path = 'C:/Users/pathouli/myStuff/academia/columbia/socialSciences/GR5067/data/topics/'\n",
    "\n",
    "the_df = pd.DataFrame()\n",
    "for dir_name in the_dirs:\n",
    "    the_filenames = os.listdir(the_path + dir_name)\n",
    "    for word in the_filenames:\n",
    "        f = open(t_path + dir_name + '/' + word, \"r\", encoding='ISO-8859-1')\n",
    "        tmp_read = str(f.read())#.encode('ISO-8859-1'))\n",
    "        tmp = pd.DataFrame([tmp_read], columns=['body'])\n",
    "        tmp['label'] = dir_name\n",
    "        the_df = the_df.append(tmp, ignore_index=True)\n",
    "        f.close()\n",
    "print (the_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body    label  \\\n",
      "0  DNR - Weekly Fishing Report\\nDNR Home Contact ...  fishing   \n",
      "1  DNR: Fishing Guide & Regulations\\nÃ Ã Header...  fishing   \n",
      "2  NH Hunting and Fishing Licenses | New Hampshir...  fishing   \n",
      "3  46 Bait & Tackle - CLOSED - Outdoor Gear - 22 ...  fishing   \n",
      "4  5 rescued from capsized sport fishing boat at ...  fishing   \n",
      "\n",
      "                                             cleaned  \n",
      "0  DNR Weekly Fishing Report DNR Home Contact DNR...  \n",
      "1  DNR Fishing Guide Regulations Header IN gov Ma...  \n",
      "2  NH Hunting and Fishing Licenses New Hampshire ...  \n",
      "3   Bait Tackle CLOSED Outdoor Gear E Columbia Av...  \n",
      "4   rescued from capsized sport fishing boat at N...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean(var):\n",
    "    tmp = re.sub('[^A-z]+', ' ', var)\n",
    "    return tmp\n",
    "\n",
    "the_df['cleaned'] = the_df.body.apply(clean)\n",
    "print (the_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0        1\n",
      "0  DNR Weekly Fishing Report DNR Home Contact DNR...  fishing\n",
      "1  DNR Fishing Guide Regulations Header IN gov Ma...  fishing\n",
      "2  NH Hunting and Fishing Licenses New Hampshire ...  fishing\n",
      "3   Bait Tackle CLOSED Outdoor Gear E Columbia Av...  fishing\n",
      "4   rescued from capsized sport fishing boat at N...  fishing\n"
     ]
    }
   ],
   "source": [
    "my_df_new_tmp_no_sw = pd.DataFrame(map(lambda x, y: (clean(x), y), the_df.body, the_df.label))\n",
    "print (my_df_new_tmp_no_sw.head())\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'class', 'is', 'lots', 'of', 'fun', 'yeah']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "the_test = 'this class is lots of fun yeah'\n",
    "\n",
    "the_tokenize = word_tokenize(the_test)\n",
    "\n",
    "print (the_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc': 2, 'b': 1, 'c': 1, 'd': 2, 'f': 3, 'g': 1, 'h': 1, 'z': 3}\n",
      "['a,b,c,d,d,d']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "the_sentence = 'abc abc b c d d f f f g h z z z'\n",
    "\n",
    "my_freq_dist = FreqDist(the_sentence.split())\n",
    "\n",
    "my_dictionary = dict(my_freq_dist)\n",
    "\n",
    "print (my_dictionary)\n",
    "\n",
    "t = 'a,b,c,d,d,d'\n",
    "my_split = t.split(',')\n",
    "print (my_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc': 2, 'b': 2, 'd': 2, 'f': 3, 'g': 1, 'h': 1, 'z': 3}\n"
     ]
    }
   ],
   "source": [
    "the_sentence = 'abc ABC b B d d F F f g h z z Z'\n",
    "\n",
    "#the_sentence = [word.lower() for word in the_sentence.split()]\n",
    "the_sentence = the_sentence.lower().split()\n",
    "\n",
    "my_freq_dist = FreqDist(the_sentence)\n",
    "\n",
    "my_dictionary = dict(my_freq_dist)\n",
    "\n",
    "print (my_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#from nltk.book import *\n",
    "\n",
    "the_books = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "print (the_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'], ['ETYMOLOGY', '.'], ...]\n",
      "10059\n"
     ]
    }
   ],
   "source": [
    "raw_text = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "#print (raw_text)\n",
    "words_text = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "#print (words_text)\n",
    "#print (len(words_text))\n",
    "words_text = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')\n",
    "print (words_text)\n",
    "print (len(words_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-6c4a780eef0d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-52-6c4a780eef0d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    the_words = [word for word in raw_text,split() if word.isdigit() or word.isalpha()]\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "the_words = [word for word in raw_text,split() if word.isdigit() or word.isalpha()]\n",
    "print (the_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'], ['ETYMOLOGY', '.'], ['(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')'], ['The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.'], ['He', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.'], ['He', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.'], ['\"', 'While', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-', 'fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', ',', 'through', 'ignorance', ',', 'the', 'letter', 'H', ',', 'which', 'almost', 'alone', 'maketh', 'the', 'signification', 'of', 'the', 'word', ',', 'you', 'deliver', 'that', 'which', 'is', 'not', 'true', '.\"'], ['--', 'HACKLUYT'], ['\"', 'WHALE', '.'], ['...', 'Sw', '.', 'and', 'Dan', '.']]\n"
     ]
    }
   ],
   "source": [
    "print (words_text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "the_books = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "import re\n",
    "import operator\n",
    "the_data = pd.DataFrame()\n",
    "for word_bk in the_books:\n",
    "    raw = nltk.corpus.gutenberg.raw(word_bk)\n",
    "    sent = nltk.corpus.gutenberg.sents(word_bk)\n",
    "    the_words = nltk.corpus.gutenberg.words(word_bk)\n",
    "    the_words = [word for word in the_words if word.isdigit() or word.isalpha()]\n",
    "    freq = dict(FreqDist(the_words))\n",
    "    top_five = sorted(freq, key=freq.get, reverse=True)[:5]\n",
    "    top_five = ',' .join(top_five)\n",
    "    bot_five = sorted(freq, key=freq.get, reverse=True)[-5:]\n",
    "    bot_five = ',' .join(bot_five)\n",
    "    word_count = [len(word) for word in sent]\n",
    "    the_data = the_data.append({'book': word_bk, \n",
    "                                'aveWordPerSent': round(np.average(word_count), 2),\n",
    "                                'medianWordPerSent': np.median(word_count),\n",
    "                                'longestSentenceLength': np.max(word_count),\n",
    "                                'shortestSentenceLength': np.min(word_count),\n",
    "                                'topFiveOccWords': top_five,\n",
    "                                'bottomFiveOccWords': bot_five}, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
